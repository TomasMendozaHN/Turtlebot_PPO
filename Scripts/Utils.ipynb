{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e85985-9aa3-456f-96e3-a84ec043e432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-4rbv3pjz because the default path (/home/jetbot/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "# Depth Camera\n",
    "import pyrealsense2.pyrealsense2 as rs\n",
    "\n",
    "# Widgets\n",
    "from IPython.display import display\n",
    "import ipywidgets\n",
    "\n",
    "# Basic libraries\n",
    "from threading import Thread\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import copy\n",
    "import time\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "# Pytorch modules\n",
    "from trt_pose.parse_objects import ParseObjects\n",
    "from trt_pose.draw_objects import DrawObjects\n",
    "import torchvision.transforms as transforms\n",
    "from torch2trt import TRTModule\n",
    "import trt_pose.models\n",
    "import torch2trt\n",
    "import trt_pose.coco\n",
    "import trt_pose\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb2c4b6-1095-4637-a0ac-b81a6419b26b",
   "metadata": {},
   "source": [
    "# Functions to prepare cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8fc22f-931c-4106-8b0c-f0a8345df14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindMyCameras():\n",
    "    ctx = rs.context()\n",
    "    devices = [x for x in list(ctx.query_devices())]\n",
    "    idxs = [str(x).find('S/N') for x in devices]\n",
    "    devices = [str(x)[y+5:y+17] for x,y in zip(devices,idxs)]\n",
    "    return devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178cff1e-2b0b-495a-bea8-f2bfbb6896c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareCamera(pipeline, serialNo, useRGB):\n",
    "    config = rs.config()\n",
    "    pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "    pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "    config.enable_device(serialNo)\n",
    "    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "    \n",
    "    if useRGB:\n",
    "        config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8456aaec-0e5d-4eae-ac03-a0d3aa25dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitializeCameras(number_of_cameras):\n",
    "    cameras = FindMyCameras()\n",
    "    frontCam = rs.pipeline()\n",
    "    frontCamConfig = PrepareCamera(frontCam, cameras[0], useRGB=True)\n",
    "    profile_front = frontCam.start(frontCamConfig)\n",
    "    depth_sensor = profile_front.get_device().first_depth_sensor()\n",
    "    front_depth_scale = depth_sensor.get_depth_scale()\n",
    "    \n",
    "    if number_of_cameras == 2:\n",
    "        backCam = rs.pipeline()\n",
    "        backCamConfig = PrepareCamera(backCam, cameras[1], useRGB=False)\n",
    "        profile_back = backCam.start(backCamConfig)\n",
    "        depth_sensor = profile_back.get_device().first_depth_sensor()\n",
    "        back_depth_scale = depth_sensor.get_depth_scale()\n",
    "        print('Cameras ready to start streaming! :)')\n",
    "        return frontCam, backCam, front_depth_scale, back_depth_scale\n",
    "    \n",
    "    print('Camera ready to start streaming! :)')\n",
    "    return frontCam, frontCam, front_depth_scale, front_depth_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b40662-fe3e-428b-94bf-28e0daa0b2ec",
   "metadata": {},
   "source": [
    "# Functions to calculate angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8349aa7-e87b-4bfe-b53c-4531c89195f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Returns angle to the robot'''\n",
    "def GetAngle(keypoints):\n",
    "    try:\n",
    "        angle = calculate_angle(640, keypoints)\n",
    "\n",
    "    except:\n",
    "        angle = [0]\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a24ed-b0d4-4fdd-800a-76b597bfff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(WIDTH, keypoints, FOV=69):\n",
    "\n",
    "    keys = keypoints.values()\n",
    "    \n",
    "    # if at least 2 points are detected\n",
    "    if len(keys) > 1:  \n",
    "        object_location = int(sum([x for x,y in keys])/len(keys))\n",
    "        angle = int(FOV*(WIDTH/2 - object_location)/WIDTH)\n",
    "        return [angle]\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        return [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c463561-3bfe-4fe2-8798-8d1575d7fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Updates Angle if there was a difference > threshold'''\n",
    "def UpdateAngle(angle, noiseThreshold):\n",
    "    \n",
    "    global global_angle\n",
    "    \n",
    "    if global_angle:\n",
    "        if abs(global_angle-angle[0])>2:\n",
    "            if angle[0] != 0:\n",
    "                global_angle = angle[0]\n",
    "\n",
    "            else:\n",
    "                global_angle = 0\n",
    "                \n",
    "    else:\n",
    "        global_angle = angle[0]   \n",
    "        \n",
    "    global_angle*=(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd2097-e91f-410a-addd-33fcff309644",
   "metadata": {},
   "source": [
    "# Functions to calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce9c42-aad2-438b-b769-15e05eaf767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Calculates the distance to the human '''\n",
    "def CalculateDistanceToHuman(depth_image, keypoints):  \n",
    "    global global_depth\n",
    "    global last_correct_distance\n",
    "    # If Human is detected, calculate Depth\n",
    "    \n",
    "    combination_shoulders = ['left_shoulder', 'right_shoulder']\n",
    "    combination_hips = ['left_hip', 'right_hip']\n",
    "    combination_elbows = ['left_elbow', 'right_elbow']\n",
    "    \n",
    "    try:\n",
    "        if len(keypoints.keys()) >= 2:\n",
    "            \n",
    "            # If both shoulders are available, use chest for distance\n",
    "            if combination_shoulders[0] in keypoints.keys() and combination_shoulders[1] in keypoints.keys():\n",
    "                center_coordinate_x = int((keypoints['left_shoulder'][0]+keypoints['right_shoulder'][0])/2)\n",
    "                center_coordinate_y = int((keypoints['left_shoulder'][1]+keypoints['right_shoulder'][1])/2)\n",
    "                depth = float(depth_image[center_coordinate_y, center_coordinate_x])\n",
    "                depth = depth#/frontScale  # use depth calibration\n",
    "                                          \n",
    "            \n",
    "            # If both shoulders are hips, use lower belly for distance\n",
    "            elif combination_hips[0] in keypoints.keys() and combination_hips[1] in keypoints.keys():\n",
    "                center_coordinate_x = int((keypoints['left_hip'][0]+keypoints['right_hip'][0])/2)\n",
    "                center_coordinate_y = int((keypoints['left_hip'][1]+keypoints['right_hip'][1])/2)\n",
    "                depth = float(depth_image[center_coordinate_y, center_coordinate_x])\n",
    "                depth = depth#/frontScale  # use depth calibration\n",
    "                \n",
    "            \n",
    "            # If both shoulders are hips, use lower belly for distance\n",
    "            elif combination_elbows[0] in keypoints.keys() and combination_elbows[1] in keypoints.keys():\n",
    "                center_coordinate_x = int((keypoints['left_elbow'][0]+keypoints['right_elbow'][0])/2)\n",
    "                center_coordinate_y = int((keypoints['left_elbow'][1]+keypoints['right_elbow'][1])/2)\n",
    "                depth = float(depth_image[center_coordinate_y, center_coordinate_x])\n",
    "                depth = depth#/frontScale  # use depth calibration\n",
    "                \n",
    "            else:\n",
    "                depth = np.mean([depth_image[keypoints[key][1],keypoints[key][0]] for key in keypoints.keys()])\n",
    "                depth = depth#/frontScale  # use depth calibration\n",
    "            \n",
    "            \n",
    "            if depth != 0:\n",
    "                last_correct_distance = depth   \n",
    "            else:\n",
    "                depth = last_correct_distance\n",
    "            \n",
    "            #depth = float(np.mean([depth_image[keypoints[x][0], keypoints[x][1]] for x in keypoints.keys()]))\n",
    "            global_depth = depth\n",
    "        \n",
    "        else:\n",
    "            global_depth = last_correct_distance\n",
    "\n",
    "    # Otherwise, set Depth = 1\n",
    "    except:\n",
    "        global_depth = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bcfb66-453d-46fb-9da3-4efc788cfda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Calculate distance reading at each cross '''\n",
    "def CalculateDistanceToRays(angles, x_coordinates, depth):\n",
    "    \n",
    "    global ray_distances\n",
    "    \n",
    "    for idx,angle in enumerate(angles):\n",
    "        \n",
    "        x = x_coordinates[idx]\n",
    "        temp_depth = depth[240, x]\n",
    "        \n",
    "        if int(abs(ray_distances[angle] - temp_depth)/5) > 10 and temp_depth >= 300:\n",
    "            \n",
    "            ray_distances[angle] = int(temp_depth)\n",
    "\n",
    "        elif int(abs(ray_distances[angle] - temp_depth)/5) > 10 and temp_depth < 300 and temp_depth > 0:\n",
    "\n",
    "            ray_distances[angle]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e6d0a-b4c0-4515-8b44-245aea6ebda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Calculates distances for backward facing rays '''\n",
    "def MeasureBackwardDistances(rays_coordinates):\n",
    "    frame = GetFrames(backwardCamera, getColor=False)\n",
    "    distances = [frame[240,x] for x in rays_coordinates]\n",
    "    distances = [round(float(x/1000), 3) for x in distances]\n",
    "    distances = [x if x <=1 else 1 for x in distances]\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c1017b-c9c8-4055-9816-b77582817123",
   "metadata": {},
   "source": [
    "# Robot Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d44168-556e-47e5-99e9-d0c2e1770f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Takes actions '''\n",
    "def step(action):\n",
    "   \n",
    "    # Extract speeds\n",
    "    try:\n",
    "        action = action.cpu().detach().numpy()[0]\n",
    "        \n",
    "    except:\n",
    "        action = action\n",
    "        \n",
    "    speed_move, speed_turn = float(abs(action[0])), float(abs(action[1]))\n",
    "    \n",
    "    speed_move = (((speed_move - 0) * (MAX_LIN_VEL - 0)) / (1 - 0)) + 0\n",
    "    speed_turn = (((speed_turn - 0) * (MAX_ANG_VEL - 0)) / (1 - 0)) + 0\n",
    "    \n",
    "    turtlebotMove(speed_move, time_move=TIME_MOVE)\n",
    "    time.sleep(TIME_MOVE + DELAY_BETWEEN_ACTIONS)  # Must include TIME_MOVE otherwise robot takes both actions simultaneoulsy\n",
    "    turtlebotRotate(speed_turn, time_rotate=TIME_ROTATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388126df-fe44-4b25-9528-74fb53c6a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Obtain actions from the Neural Network '''\n",
    "def sample_action(observation):\n",
    "    \n",
    "    global agent\n",
    "    \n",
    "    observation = torch.Tensor(observation).cuda()\n",
    "    hidden,_ = agent.network_body(vis_inputs=[0],vec_inputs=[observation])\n",
    "    distribution = agent.distribution(hidden)[0]\n",
    "    action = distribution.sample()\n",
    "    action = torch.clamp(action, -3, 3)/3\n",
    "    \n",
    "    return action    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6067f0e-6ccb-4130-8861-0ed1cf25efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Use Neural Network to move robot '''\n",
    "def MoveRobot(observation):\n",
    "    # Move if person is more than 0.8 distance units away\n",
    "    if observation[3] < 0.6 and observation[0]==1:\n",
    "        action = [0, 0]\n",
    "\n",
    "    else:\n",
    "        action = sample_action(observation)\n",
    "        step(action)\n",
    "\n",
    "        # Extract action for printing info\n",
    "        action = action.cpu().detach().numpy()[0]\n",
    "        speed_move, speed_turn = float(abs(action[0])), float(abs(action[1]))\n",
    "        action = [speed_move, speed_turn]\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc55c692-bf27-4e7d-bb77-87e34df36083",
   "metadata": {},
   "source": [
    "# Functions related to frame processing & drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54122aa2-c1dc-448f-9aba-0a2bd284e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgr8_to_jpeg(value, quality=75):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b716624-ae15-42d9-8a15-88b4b244ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    global device\n",
    "    image = cv2.resize(image, (224,224))\n",
    "    device = torch.device('cuda')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device)\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bb8425-9782-4793-bbf0-0dd59d9ae332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints(image, human_pose, topology, object_counts, objects, normalized_peaks):\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    keypoints = {}\n",
    "    K = topology.shape[0]\n",
    "    count = int(object_counts[0])\n",
    "\n",
    "    for i in range(count):\n",
    "        obj = objects[0][i]\n",
    "        C = obj.shape[0]\n",
    "        for j in range(C):\n",
    "            k = int(obj[j])\n",
    "            if k >= 0:\n",
    "                peak = normalized_peaks[0][j][k]\n",
    "                x = round(float(peak[1]) * width)\n",
    "                y = round(float(peak[0]) * height)\n",
    "                keypoints[human_pose[\"keypoints\"][j]] = (x, y)\n",
    "\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb284279-7ddc-4d06-8f29-23d1d265de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Draw skeleton and rays on the frames '''\n",
    "def DrawOnFrames(color_image, counts, objects, peaks):   \n",
    "    global image_to_nn\n",
    "    draw_objects(color_image, counts, objects, peaks)    \n",
    "    color_image = draw_rays(color_image, rays_coordinates, rays_in_angles, ray_distances)\n",
    "    image_to_nn = copy.deepcopy(color_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4843e-bc20-49af-ac04-661055a143b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Reads One Camera and returns Frames as arrays'''\n",
    "def GetFrames(camera, getColor=False):\n",
    "    align_to = rs.stream.color\n",
    "    align = rs.align(align_to)\n",
    "    frames = camera.wait_for_frames()\n",
    "    aligned_frames = align.process(frames)\n",
    "    depth_frame = aligned_frames.get_depth_frame()\n",
    "    depth_image = np.asanyarray(depth_frame.get_data())\n",
    "\n",
    "    if getColor:\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.flip(color_image, 1)\n",
    "        return color_image, depth_image\n",
    "    else:\n",
    "        return depth_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa4fd0-4d28-4416-b93a-41023ab3ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateKeyPoints(color_image):\n",
    "    data = preprocess(color_image)\n",
    "    cmap, paf = model_trt(data)\n",
    "    cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "    counts, objects, peaks = parse_objects(cmap, paf)\n",
    "    keypoints = get_keypoints(color_image, human_pose, topology, counts, objects, peaks)\n",
    "    return keypoints, counts, objects, peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33bf2b-d2ca-42be-933a-984f2de259f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Draw the crosses on the left frame (just for reference) '''\n",
    "def draw_rays(image, ray_positions, angles, distances):\n",
    "    \n",
    "    # Mirror image so text written is correct\n",
    "    image = cv2.flip(image, 1)\n",
    "    \n",
    "    red_color = (0,0,255)\n",
    "    yellow_color = (0,255,255)\n",
    "    green_color = (0,255,0)\n",
    "    \n",
    "    # Draw a '+' at every ray position & write the angle that cross belongs to\n",
    "    for idx,ray in enumerate(ray_positions):\n",
    "        \n",
    "        # Assign cross color based on distance\n",
    "        distance_to_object = distances[angles[idx]]/10\n",
    "        cross_color = lambda x: yellow_color if x< 80 and x >=50 else red_color if x < 50 else green_color\n",
    "        color = cross_color(distance_to_object)\n",
    "\n",
    "        # Draw the cross\n",
    "        cv2.line(image, (ray+3,240), (ray-3,240), color, 2)    # Horizontal line\n",
    "        cv2.line(image, (ray, 240-3), (ray, 240+3), color, 2)  # Vertical line\n",
    "        \n",
    "        # Write what angle the cross belongs to        \n",
    "        cv2.putText(image, str(angles[idx]), org=(ray-15, 240+22), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=color, thickness=1)\n",
    "        \n",
    "    # Return to normal\n",
    "    image = cv2.flip(image, 1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a651b-9b55-479d-a957-9e3cb1a25a7a",
   "metadata": {},
   "source": [
    "# Functions to handle the UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542f11a-b34a-4cf2-b2cf-8c738a3038ee",
   "metadata": {},
   "source": [
    "### General UI Functions (both left and right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe9e3a9-bac5-4873-9b13-0fce5fd7f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create the UI '''\n",
    "def CreateUI():\n",
    "    \n",
    "    global image_w, rays_widget, distance_widget, angle_widget\n",
    "    global RL_image_w, RL_rays_widget, RL_distance_widget, RL_angle_widget, RL_stage_actions_widget\n",
    "    \n",
    "    # Real Time Widgets\n",
    "    image_w = ipywidgets.Image(format='jpeg', width=480, height=480)\n",
    "    rays_widget = ipywidgets.HTML()\n",
    "    distance_widget = ipywidgets.HTML()\n",
    "    angle_widget = ipywidgets.HTML()\n",
    "\n",
    "    text_widget = ipywidgets.VBox([distance_widget, rays_widget, angle_widget], layout=ipywidgets.Layout(width='100%'))\n",
    "    final_real_time = ipywidgets.HBox([image_w, text_widget], layout=ipywidgets.Layout(width='100%', align_self='center'))\n",
    "    final_real_time.add_class(\"box_style\")\n",
    "\n",
    "    # RL widgets\n",
    "    RL_image_w = ipywidgets.Image(format='jpeg', width=480, height=480)\n",
    "    RL_rays_widget = ipywidgets.HTML()\n",
    "    RL_distance_widget = ipywidgets.HTML()\n",
    "    RL_angle_widget = ipywidgets.HTML()\n",
    "\n",
    "    RL_text_widget = ipywidgets.VBox([RL_distance_widget, RL_rays_widget, RL_angle_widget], layout=ipywidgets.Layout(width='100%'))\n",
    "    RL_final_real_time = ipywidgets.HBox([RL_text_widget, RL_image_w], layout=ipywidgets.Layout(width='100%', align_self='center'))\n",
    "    RL_final_real_time.add_class(\"RL_box_style\")\n",
    "\n",
    "    RL_stage_actions_widget = ipywidgets.HTML()\n",
    "    RL_stage_actions_widget.add_class(\"RL_stage_actions_style\")\n",
    "\n",
    "    results = ipywidgets.HBox([RL_final_real_time, final_real_time], layout=ipywidgets.Layout(width='100%', align_self='center'))\n",
    "    results = ipywidgets.VBox([results, RL_stage_actions_widget], layout=ipywidgets.Layout(width='100%'))\n",
    "    \n",
    "    display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509db0d5-8a47-47c8-a4d9-e3d9ee90c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".RL_box_style{\n",
    "    width:auto;\n",
    "    border : 1px solid red;\n",
    "    height: auto;\n",
    "    background-color:Thistle;\n",
    "}\n",
    ".box_style{\n",
    "    width:auto;\n",
    "    border : 1px solid green;\n",
    "    height: auto;\n",
    "    background-color:PaleGreen;\n",
    "}\n",
    ".RL_stage_actions_style{\n",
    "    width:auto;\n",
    "    border : 1px solid blue;\n",
    "    height: auto;\n",
    "    background-color:LightCyan;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9fa877-6fa1-4077-8e92-3e8434c34719",
   "metadata": {},
   "source": [
    "### For the Widgets of the right side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dac834-f1d5-4774-b0be-3657409add88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UpdateWidgets(rays_in_angles): \n",
    "    rays_widget.value = f\"<pref><font color='blue'>Distance to each ray (Raw Readings)<br> {rays_in_angles[0]}: {ray_distances[rays_in_angles[0]]/10}cm<br> {rays_in_angles[1]}: {ray_distances[rays_in_angles[1]]/10}cm<br> {rays_in_angles[2]}: {ray_distances[rays_in_angles[2]]/10}cm<br> {rays_in_angles[3]}: {ray_distances[rays_in_angles[3]]/10}cm<br> {rays_in_angles[4]}: {ray_distances[rays_in_angles[4]]/10}cm<br> {rays_in_angles[5]}: {ray_distances[rays_in_angles[5]]/10}cm<br> {rays_in_angles[6]}: {ray_distances[rays_in_angles[6]]/10}cm<br> {rays_in_angles[7]}: {ray_distances[rays_in_angles[7]]/10}cm<br> {rays_in_angles[8]}: {ray_distances[rays_in_angles[8]]/10}cm<br> {rays_in_angles[9]}: {ray_distances[rays_in_angles[9]]/10}cm<br> {rays_in_angles[10]}: {ray_distances[rays_in_angles[10]]/10}cm<br> {rays_in_angles[11]}: {ray_distances[rays_in_angles[11]]/10}cm<br> {rays_in_angles[12]}: {ray_distances[rays_in_angles[12]]/10}cm<br>\"             \n",
    "    distance_widget.value = f\"<pref><font color='red'>Distance to person: {int(global_depth/10)}cm\"\n",
    "    angle_widget.value = f\"<pref><font color='purple'>Angle to person: {int(global_angle)}\"\n",
    "    image_w.value = bgr8_to_jpeg(color_image[:, ::-1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb82f8b-545e-49f2-beec-511abd1af0aa",
   "metadata": {},
   "source": [
    "### For the Widgets of the left side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb457cc-1696-485c-ac46-f70279d4ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Normalizes the widget values '''\n",
    "def NormalizeWidgetValues():\n",
    "    angles_to_NN = [round(float(ray_distances[x]/1000), 3) for x in ray_distances]\n",
    "    angles_to_NN = [x if x <=1 else 1 for x in angles_to_NN ]\n",
    "    distance_to_person_NN = float(global_depth/1000)\n",
    "    if distance_to_person_NN >= 3:\n",
    "        distance_to_person_NN = 1\n",
    "    else:\n",
    "        distance_to_person_NN=round(distance_to_person_NN/3,2)\n",
    "        \n",
    "    return distance_to_person_NN, angles_to_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b441ace2-af63-4dbc-8184-1a6ef348afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Updates widgets on the left '''\n",
    "def UpdateDelayedWidgets(distance_to_person_NN, angles_to_NN):\n",
    "    RL_rays_widget.value = f\"<pref><font color='blue'>Distance to each ray (normalized) <br> {rays_in_angles[0]}: {angles_to_NN[0]}<br> {rays_in_angles[1]}: {angles_to_NN[1]}<br> {rays_in_angles[2]}: {angles_to_NN[2]}<br> {rays_in_angles[3]}: {angles_to_NN[3]}<br> {rays_in_angles[4]}: {angles_to_NN[4]}<br> {rays_in_angles[5]}: {angles_to_NN[5]}<br> {rays_in_angles[6]}: {angles_to_NN[6]}<br> {rays_in_angles[7]}: {angles_to_NN[7]}<br> {rays_in_angles[8]}: {angles_to_NN[8]}<br> {rays_in_angles[9]}: {angles_to_NN[9]}<br> {rays_in_angles[10]}: {angles_to_NN[10]}<br> {rays_in_angles[11]}: {angles_to_NN[11]}<br> {rays_in_angles[12]}: {angles_to_NN[12]}<br>\"             \n",
    "    RL_distance_widget.value = f\"<pref><font color='red'>Distance to person: {distance_to_person_NN}m\"\n",
    "    RL_angle_widget.value = f\"<pref><font color='purple'>Angle to person: {int(global_angle)}\"\n",
    "    RL_image_w.value = bgr8_to_jpeg(image_to_nn[:, ::-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded2d187-6f75-4f2a-a058-3a65327ef8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Calculates the stage '''\n",
    "def DetermineStage():\n",
    "    # Define Stage\n",
    "    try:\n",
    "        # If everything is visible, stage = 1\n",
    "        if 'left_ankle' in keypoints.keys() and 'right_ankle' in keypoints.keys() and 'left_knee' in keypoints.keys() and 'right_knee' in keypoints.keys():\n",
    "            stage = [1,0,0]\n",
    "\n",
    "        # Else, stage = 2\n",
    "        else:\n",
    "            if len(keypoints.keys()) >= 2:\n",
    "                stage = [0,1,0]\n",
    "            else:\n",
    "                stage = [0,0,1]\n",
    "    except:\n",
    "        stage = [0,0,1]\n",
    "\n",
    "    return stage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
