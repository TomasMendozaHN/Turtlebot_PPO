{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd24d54-589f-4e5c-a854-113e1a89d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (7,12)\n",
    "\n",
    "# Step history\n",
    "steps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340c26e-eb75-481c-bdd3-f448b572e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNNInUnity():\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        self.agent = torch.load(model_path)\n",
    "        self.agent.eval()\n",
    "        print(\"1. Model Loaded.\")\n",
    "        \n",
    "        print(\"2. Connecting Unity Environment...\")\n",
    "        self.env = UnityEnvironment(file_name= None, base_port=5004) \n",
    "        self.env.reset()\n",
    "        print(\"3. Connected.\")\n",
    "        \n",
    "        self.behaviorNames = list(self.env.behavior_specs.keys())\n",
    "        self.behaviorName = self.behaviorNames[0]\n",
    "        self.behavior_spec = self.env.behavior_specs[self.behaviorName]\n",
    "        self.num_inputs  = self.behavior_spec.observation_shapes[0][0]\n",
    "        self.num_outputs = self.behavior_spec.action_spec[0]\n",
    "        print(\"4. Connection with Unity is ready!\")\n",
    "        \n",
    "        if(torch.cuda.is_available()):\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device= torch.device(\"cpu\")\n",
    "        print(\"5. Going to use device = \", self.device)\n",
    "       \n",
    "    \n",
    "        \n",
    "    '''  \n",
    "    ########################################  Test Unity Environment  ########################################\n",
    "    '''    \n",
    "    def test(self, episodes_to_collect=10, max_steps_per_episode=1000):\n",
    "        self.episodes_to_collect = episodes_to_collect\n",
    "        self.max_steps_per_episode = max_steps_per_episode \n",
    "        print(\"6. Testing your trained model...\")\n",
    "        with torch.no_grad():\n",
    "            total_Steps = []\n",
    "            for game in range(self.episodes_to_collect):\n",
    "                for step in range(self.max_steps_per_episode): \n",
    "\n",
    "                    # Get Observation\n",
    "                    step_result = self.env.get_steps(self.behaviorName) \n",
    "                    DecisionSteps = step_result[0] \n",
    "                    TerminalSteps = step_result[1]\n",
    "\n",
    "                    if len (TerminalSteps.obs[0]) > 0:\n",
    "                        self.env.reset()\n",
    "                        total_Steps.append(step)\n",
    "                        print(f'Episode #{game}. Avg # of steps so far ==> {np.mean(total_Steps)}')\n",
    "                        break\n",
    "                    \n",
    "                    state = DecisionSteps.obs[0][0]\n",
    "                    state = torch.FloatTensor(state).to(self.device)\n",
    "\n",
    "                    # Get Steps\n",
    "                    hidden, _ = self.agent.get_dists([state], [], masks=torch.ones((1,6)))\n",
    "                    actions = []\n",
    "                    for dist in hidden:\n",
    "                        actions.append(dist.sample().cpu().numpy()[0][0])\n",
    "\n",
    "                    self.env.set_actions(self.behaviorName, np.array([actions]))\n",
    "                    self.env.step()\n",
    "\n",
    "            print(f\"7. Done. Average steps = {np.mean(total_Steps)}\\n\\n\")\n",
    "            return total_Steps\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    '''  \n",
    "    ########################################  Test Unity Environment with modified probability  ########################################\n",
    "    ############################# use this only if you want to take different actions than trained policy ##############################\n",
    "    '''\n",
    "    def test_Modified(self, episodes_to_collect=10, max_steps_per_episode=1000, prob=0.9):\n",
    "        self.episodes_to_collect = episodes_to_collect\n",
    "        self.max_steps_per_episode = max_steps_per_episode \n",
    "        self.probability = prob\n",
    "        \n",
    "        self.trajectories = {}\n",
    "        \n",
    "        print(\"6. Testing your trained model...\")\n",
    "        with torch.no_grad():\n",
    "            total_Steps = []\n",
    "            for game in range(self.episodes_to_collect):\n",
    "                self.trajectories[game]=[]\n",
    "                for step in range(self.max_steps_per_episode): \n",
    "\n",
    "                    # Get Observation\n",
    "                    step_result = self.env.get_steps(self.behaviorName) \n",
    "                    DecisionSteps = step_result[0] \n",
    "                    TerminalSteps = step_result[1]\n",
    "\n",
    "                    if len (TerminalSteps.obs[0]) > 0:\n",
    "                        self.env.reset()\n",
    "                        total_Steps.append(step)\n",
    "                        print(f'Episode #{game}. Avg # of steps so far ==> {np.mean(total_Steps)}')\n",
    "                        self.trajectories[game] = np.array(self.trajectories[game])\n",
    "                        break\n",
    "                    \n",
    "                    state = DecisionSteps.obs[0][0]\n",
    "                    self.trajectories[game].append([(state[1]+0.25)*-1, (state[2]+3.64)*-1])\n",
    "                    #print(f\"X = {state[1]+0.25}, Z = {state[2]+3.64}\")\n",
    "                    state = torch.FloatTensor(state).to(self.device)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    # Get Steps\n",
    "                    actions = []\n",
    "                    dists = self.agent.get_dists([state], [], masks=torch.ones((1,6)))\n",
    "                    percentage = 0.9\n",
    "                    for dist in dists[0]:\n",
    "                        probs = dist.probs.cpu().detach().numpy()\n",
    "                        diff = (np.max(probs)-(np.max(probs)*self.probability))/2\n",
    "                        probs_ = [[x+diff if x != np.max(probs) else np.max(probs)*self.probability for x in probs[0]]]\n",
    "                        actions.append(torch.multinomial(torch.tensor(probs_).to(self.device), 1).cpu().numpy()[0][0])\n",
    "\n",
    "                        \n",
    "                    self.env.set_actions(self.behaviorName, np.array([actions]))\n",
    "                    self.env.step()\n",
    "\n",
    "            print(f\"7. Done. Average steps = {np.mean(total_Steps)}\\n\\n\")\n",
    "            return total_Steps, self.trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b161f2-575a-4daa-8a4a-0b3362e68bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = TestNNInUnity(\"Turtlebot3_0331.pth\")\n",
    "\n",
    "'''\n",
    "Use the line below if you want to use the trained policy, WITHOUT ANY PROBABILITY MODIFICATIONS\n",
    "'''\n",
    "steps.append(tester.test(episodes_to_collect=100, max_steps_per_episode=10000)) \n",
    "\n",
    "\n",
    "'''  \n",
    "Use the line below if you want to MODIFY THE PROBABILITY.\n",
    "prob ===>  if you set prob = 0.9,  it means we will reduce the maximum probability by 10% \n",
    "'''\n",
    "#steps, trajectories = tester.test_Modified(games_to_play=100, max_steps_per_episode=10000, prob=0.6)\n",
    "print(\"steps = \", steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0021b919-ea06-4c1e-b8d7-7034a1146534",
   "metadata": {},
   "source": [
    "# Read trajectories from CSV and graph them as a transparent PNG\n",
    "### ***** IMPORTANT: Only use when the \"test\" function is used! ****\n",
    "### ***** DO NOT USE IF YOU USED THE test_Modified function! *****\n",
    "###  If you use the test_modified function, skip to the last code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa12d5-1384-4575-89d8-798e3e98cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pth = \"M:\\ML Agents\\envs\\CarDiscreteLargeMaze\\LSTM_Stacked_large.csv\"\n",
    "df = pd.read_csv(pth)\n",
    "array = np.array(df.iloc[: , :2])\n",
    "counter = 0\n",
    "trajectories = {}\n",
    "for row in array:\n",
    "    if row[0] != 0:\n",
    "        if counter not in trajectories.keys():\n",
    "            trajectories[counter]=[]\n",
    "        trajectories[counter].append([float(x) for x in row])\n",
    "    \n",
    "    else:\n",
    "        trajectories[counter] = np.array(trajectories[counter])\n",
    "        counter+=1\n",
    "\n",
    "        \n",
    "''' \n",
    "this will save the trajectory as a transparent PNG, ready to paste on top of a picture of your environment!\n",
    "'''\n",
    "plt.rcParams['figure.figsize'] = (7,12)\n",
    "for _,traj in trajectories.items():\n",
    "    plt.axis('off')\n",
    "    plt.plot(traj[:,0],traj[:,1], color='black')\n",
    "    # for x,y in traj:\n",
    "    #     plt.plot(x,y, color='blue')\n",
    "plt.savefig(\"fig.png\", transparent=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501506cf-19a5-426a-8ebd-80fba9af84ed",
   "metadata": {},
   "source": [
    "# Graph the trajectories\n",
    "# IMP: Only use when \"test_Modified\" is used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c43fb-51a7-4bb5-a00f-6d4e54d12260",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b323c-04bd-404d-90fd-2d292506d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "this will save the trajectory as a transparent PNG, ready to paste on top of a picture of your environment!\n",
    "'''\n",
    "plt.rcParams['figure.figsize'] = (7,12)\n",
    "for _,traj in trajectories.items():\n",
    "    plt.axis('off')\n",
    "    plt.plot(traj[:,0],traj[:,1], color='black')\n",
    "    # for x,y in traj:\n",
    "    #     plt.plot(x,y, color='blue')\n",
    "plt.savefig(\"fig.png\", transparent=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
