{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad871e4-a9fb-47af-b62d-a710e1cba330",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Scripts/Utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d566ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get error importing rospy, carefully \n",
    "%run Scripts/TurtlebotMovement.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29db1d2-d113-418f-98f7-b7f5c8269f4b",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22704705-a7e4-4919-ad9e-4e759523e148",
   "metadata": {},
   "source": [
    "### Video Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deae951e-c8a0-43fa-af94-b9e514e79fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If True, wheels will NOT move. \n",
    "TEST_VIDEO_ONLY = False\n",
    "\n",
    "# How many cameras you have connected\n",
    "NUMBER_OF_CAMERAS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a9c71d-67a5-4b61-afa7-42f5502ebbd6",
   "metadata": {},
   "source": [
    "#### Parameters related to the State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b5ba8-3323-4ca0-b3da-7998ee669655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Angles at which Frontal camera will read distance (like range sensors in Unity)\n",
    "rays_in_angles   = [-30, -25, -20, -15, -10, -5, 0, 5, 10, 15, 20, 25, 30]\n",
    "rays_coordinates = [42, 88, 135, 181, 228, 274, 320, 366, 412, 459, 505, 552, 598]  # X coordinates of pixels\n",
    "ray_distances = {k:-1 for k in rays_in_angles}\n",
    "\n",
    "# Angles at which Backward camera will read distance\n",
    "back_rays_coordinates = [134,227,320,413,506]  # 160,170,180,190,200 deg\n",
    "\n",
    "# To avoid getting distance = 0 when camera misreads the distance to human\n",
    "last_correct_distance = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33314b-4a82-4046-91e5-76e77587cb49",
   "metadata": {},
   "source": [
    "#### Engine Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481952e-85a1-45cb-98aa-2cc7dffbab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To control the motors!\n",
    "global TIME_MOVE, TIME_ROTATE, MAX_LIN_VEL, MAX_ANG_VEL, DELAY_BETWEEN_ACTIONS\n",
    "\n",
    "if TEST_VIDEO_ONLY:\n",
    "    TIME_MOVE = 0\n",
    "    TIME_ROTATE = 0\n",
    "    MAX_LIN_VEL = 0\n",
    "    MAX_ANG_VEL = 0\n",
    "    DELAY_BETWEEN_ACTIONS = 0 \n",
    "    \n",
    "else:\n",
    "    TIME_MOVE = 0.5      # Amount of seconds performing a movement.\n",
    "    TIME_ROTATE = 0.2    # Amount of seconds performing a rotation.\n",
    "    MAX_LIN_VEL = 0.40   # Default = 0.4. Do not make higher, only lower is ok.\n",
    "    MAX_ANG_VEL = 2.79   # Default = 2.79. Do not make higher, only lower is ok.\n",
    "    DELAY_BETWEEN_ACTIONS = 0.2  # Amount of seconds between end of MOVE and start of ROTATE. If = 0, movement might be unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d669f78c-d6d9-4d4f-97b7-41b43a0ad9fd",
   "metadata": {},
   "source": [
    "# Load Pose Estimation network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dfc543-d539-4989-919b-daf8e7c1b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KeyPoints TRT Model Path\n",
    "OPTIMIZED_MODEL = 'Files/resnet18_baseline_att_224x224_A_epoch_249_trt_NX.pth'\n",
    "\n",
    "# Topology\n",
    "with open('Files/human_pose.json', 'r') as f:\n",
    "    human_pose = json.load(f)\n",
    "topology = trt_pose.coco.coco_category_to_topology(human_pose)\n",
    "\n",
    "# load Model\n",
    "WIDTH = 224\n",
    "HEIGHT = 224\n",
    "data = torch.zeros((1, 3, HEIGHT, WIDTH)).cuda()\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load(OPTIMIZED_MODEL))\n",
    "print(\"KeyPoints RCNN Successfully Loaded. \")\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ccd3db-24c8-4d48-af4b-5679c95c4633",
   "metadata": {},
   "source": [
    "# Define functions to draw skeleton on frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a716a3f9-6d27-434c-bef0-d2947a5337b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_objects = ParseObjects(topology, cmap_threshold=0.3, link_threshold=0.2)\n",
    "draw_objects = DrawObjects(topology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7eb57-5cb7-49b8-8012-d53f183d606f",
   "metadata": {},
   "source": [
    "# Load Robot Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802fef05-2a11-4c18-956e-d2b5bb49bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_brain_pth = \"Files/MobileRobot-8000767.pth\"\n",
    "agent = torch.load(agent_brain_pth)\n",
    "agent.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ba520-068c-4372-828d-d41da48ee0a5",
   "metadata": {},
   "source": [
    "# Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ef14ff-c7e0-4d64-8a8e-f7e987f56b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robot_take_action():\n",
    "    global image_to_nn    # frame\n",
    "    global global_angle   # angle to person\n",
    "    global ray_distances  # ray sensor readings\n",
    "    global global_depth   # distance to person\n",
    "    global keypoints\n",
    "    global running_thread\n",
    "    running_thread = True\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # Visualize on left Widgets\n",
    "        distance_to_person_NN, angles_to_NN = NormalizeWidgetValues()\n",
    "        UpdateDelayedWidgets(distance_to_person_NN, angles_to_NN)\n",
    "        \n",
    "        # Stage 1 = [1,0,0] ==> Can see ankles and knees\n",
    "        # Stage 2 = [0,1,0] ==> Can see human but not ankles or knees (we assume this means obstacle is between)\n",
    "        # Stage 3 = [0,0,1] ==> Can't see anyone\n",
    "        stage = DetermineStage()\n",
    "        \n",
    "        # Read backward rays\n",
    "        distances = MeasureBackwardDistances(back_rays_coordinates)\n",
    "        \n",
    "        # Prepare observation as list\n",
    "        observation = stage + [distance_to_person_NN, global_angle] + angles_to_NN + distances\n",
    "\n",
    "        # Move robot and return action\n",
    "        action = MoveRobot(observation)\n",
    "        step(action)\n",
    "            \n",
    "        # Update last widget\n",
    "        RL_stage_actions_widget.value = f\"<pref><font color='red'>Action Move = {round(action[0], 2)}.    Action Turn = {round(action[1], 2)}. <br> Stage = {stage}. <br>  Observation = {observation}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f980b8-37de-4e05-8623-97c97c66d29e",
   "metadata": {},
   "source": [
    "# Control Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c87ffb2-b8f1-4cad-9ad9-51665b020a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "\n",
    "    # For second thread\n",
    "    global running_thread # To avoid thread running multiple times\n",
    "    global color_image    # frame\n",
    "    global global_angle   # angle to person\n",
    "    global ray_distances  # ray sensor readings\n",
    "    global global_depth   # distance to person\n",
    "    global image_to_nn\n",
    "    global robot\n",
    "    global keypoints\n",
    "    global image_w, rays_widget, distance_widget, angle_widget\n",
    "    global_angle = 0\n",
    "    running_thread = False\n",
    "    \n",
    "    # Streaming loop\n",
    "    while True:\n",
    "        \n",
    "        # Read Camera\n",
    "        color_image, depth_image = GetFrames(camera = frontalCamera, getColor=True)\n",
    "\n",
    "        # Calculate KeyPoints Skeleton\n",
    "        keypoints, counts, objects, peaks = CalculateKeyPoints(color_image)\n",
    "        \n",
    "        # Draw detected KeyPoints on human + Distance Rays crosses (that change color)\n",
    "        DrawOnFrames(color_image, counts, objects, peaks)     \n",
    "        \n",
    "        # Calculate Distance to Human (result ==> global_depth)\n",
    "        CalculateDistanceToHuman(depth_image, keypoints)\n",
    "        \n",
    "        # Calculate distance at each ray (result ==> ray_distances)\n",
    "        CalculateDistanceToRays(rays_in_angles, rays_coordinates, depth_image)\n",
    "        \n",
    "        # Calculate & Update the Angle \n",
    "        angle = GetAngle(keypoints)\n",
    "        UpdateAngle(angle, noiseThreshold=2)\n",
    "        \n",
    "        # Update Real-Time Widgets (Widgets with a delay are updated on the thread)\n",
    "        UpdateWidgets(rays_in_angles)\n",
    "        \n",
    "        if not running_thread:\n",
    "            thread = Thread(target = robot_take_action)\n",
    "            thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46afed0-f27c-4e2a-bd1f-128c8b0230b9",
   "metadata": {},
   "source": [
    "# Initialize Cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29864e10-b7bd-4c7c-9c8a-c62ddaff8c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "frontalCamera, backwardCamera, frontScale, backScale = InitializeCameras(NUMBER_OF_CAMERAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b46c9e-83bb-40bf-9cec-2349b94f876e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create video widget & Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6902a-0143-4b6d-9588-858c559b23ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "CreateUI()\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
